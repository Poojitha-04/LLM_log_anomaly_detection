{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import re\n",
    "import json\n",
    "import hashlib\n",
    "import pandas as pd\n",
    "from collections import defaultdict\n",
    "from tqdm import tqdm\n",
    "import pickle\n",
    "from collections import Counter\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "import numpy as np\n",
    "import random\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class DrainParser:\n",
    "    def __init__(self, log_format, indir, outdir, depth=4, st=0.5, rex=None):\n",
    "        self.path = indir\n",
    "        self.savePath = outdir\n",
    "        self.depth = depth - 2\n",
    "        self.st = st\n",
    "        self.rex = rex or []\n",
    "        self.log_format = log_format\n",
    "\n",
    "    def preprocess(self, line):\n",
    "        for regex in self.rex:\n",
    "            line = re.sub(regex, '<*>', line)\n",
    "        return line\n",
    "\n",
    "    def generate_logformat_regex(self, logformat):\n",
    "        headers = []\n",
    "        splitters = re.split(r'(<[^<>]+>)', logformat)\n",
    "        regex = ''\n",
    "        for i, part in enumerate(splitters):\n",
    "            if i % 2 == 0:\n",
    "                regex += re.sub(r' +', r'\\\\s+', part)\n",
    "            else:\n",
    "                header = part.strip('<>').strip()\n",
    "                headers.append(header)\n",
    "                regex += f'(?P<{header}>.*?)'\n",
    "        return headers, re.compile('^' + regex + '$')\n",
    "\n",
    "    def load_data(self, log_file):\n",
    "        headers, regex = self.generate_logformat_regex(self.log_format)\n",
    "        rows = []\n",
    "        with open(log_file, 'r') as f:\n",
    "            for line in f:\n",
    "                try:\n",
    "                    match = regex.search(line.strip())\n",
    "                    rows.append([match.group(h) for h in headers])\n",
    "                except:\n",
    "                    continue\n",
    "        df = pd.DataFrame(rows, columns=headers)\n",
    "        df.insert(0, 'LineId', range(1, len(df) + 1))\n",
    "        return df\n",
    "\n",
    "    def parse(self, log_name):\n",
    "        full_path = os.path.join(self.path, log_name)\n",
    "        print(\"üîç Parsing log file:\", full_path)\n",
    "\n",
    "        df_log = self.load_data(full_path)\n",
    "        df_log['Content'] = df_log['Content'].apply(self.preprocess)\n",
    "\n",
    "        templates = df_log['Content'].value_counts().reset_index()\n",
    "        templates.columns = ['EventTemplate', 'Occurrences']\n",
    "        templates['EventId'] = templates['EventTemplate'].apply(lambda x: hashlib.md5(x.encode()).hexdigest()[:8])\n",
    "\n",
    "        id_map = templates.set_index('EventTemplate')['EventId'].to_dict()\n",
    "        df_log['EventTemplate'] = df_log['Content']\n",
    "        df_log['EventId'] = df_log['EventTemplate'].map(id_map)\n",
    "\n",
    "        os.makedirs(self.savePath, exist_ok=True)\n",
    "        df_log.to_csv(os.path.join(self.savePath, log_name + '_structured.csv'), index=False)\n",
    "        templates.to_csv(os.path.join(self.savePath, log_name + '_templates.csv'), index=False)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def extract_templates_to_json(template_csv, output_json):\n",
    "    df = pd.read_csv(template_csv)\n",
    "    df.sort_values(by=\"Occurrences\", ascending=False, inplace=True)\n",
    "    template_map = {row[\"EventId\"]: i + 1 for i, row in df.iterrows()}\n",
    "\n",
    "    with open(output_json, \"w\") as f:\n",
    "        json.dump(template_map, f)\n",
    "\n",
    "    print(f\"Template map saved to {output_json}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def extract_sessions(structured_csv, template_map_json, output_csv):\n",
    "    df = pd.read_csv(structured_csv)\n",
    "\n",
    "    with open(template_map_json, 'r') as f:\n",
    "        template_map = json.load(f)\n",
    "\n",
    "    df[\"EventId\"] = df[\"EventId\"].map(template_map)\n",
    "\n",
    "    blk_map = defaultdict(list)\n",
    "    for _, row in df.iterrows():\n",
    "        blk_ids = re.findall(r'blk_-?\\d+', row[\"Content\"])\n",
    "        for blk in set(blk_ids):\n",
    "            blk_map[blk].append(row[\"EventId\"])\n",
    "\n",
    "    session_df = pd.DataFrame(list(blk_map.items()), columns=[\"BlockId\", \"EventSequence\"])\n",
    "    session_df.to_csv(output_csv, index=False)\n",
    "\n",
    "    print(f\" Sessions extracted to {output_csv}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def split_train_test(sequence_csv, label_csv, output_dir, max_train=None, ratio=0.3):\n",
    "    df_seq = pd.read_csv(sequence_csv)\n",
    "    df_label = pd.read_csv(label_csv)\n",
    "\n",
    "    label_map = dict(zip(df_label[\"BlockId\"], df_label[\"Label\"].map(lambda x: 1 if x == \"Anomaly\" else 0)))\n",
    "    df_seq[\"Label\"] = df_seq[\"BlockId\"].map(label_map)\n",
    "\n",
    "    normal = df_seq[df_seq[\"Label\"] == 0][\"EventSequence\"].dropna().sample(frac=1, random_state=42)\n",
    "    anomaly = df_seq[df_seq[\"Label\"] == 1][\"EventSequence\"].dropna()\n",
    "\n",
    "    if max_train is None:\n",
    "        max_train = int(len(normal) * ratio)\n",
    "\n",
    "    datasets = {\n",
    "        \"train\": normal[:max_train],\n",
    "        \"test_normal\": normal[max_train:],\n",
    "        \"test_abnormal\": anomaly\n",
    "    }\n",
    "\n",
    "    os.makedirs(output_dir, exist_ok=True)\n",
    "    for name, data in datasets.items():\n",
    "        with open(os.path.join(output_dir, name), \"w\") as f:\n",
    "            for seq in data:\n",
    "                f.write(\" \".join(map(str, eval(seq))) + \"\\n\")\n",
    "\n",
    "    print(\"Train/test split completed.\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "input_dir = \"/Users/poojithareddy/Desktop/ucf/Sem2/NLP/nlp_project/dataset/HDFS/\"\n",
    "output_dir = \"output_hdfs/\"\n",
    "log_file = \"HDFS.log\"\n",
    "log_format = \"<Date> <Time> <Pid> <Level> <Component>: <Content>\"\n",
    "\n",
    "# Step 1: Drain Parsing (NO regex to preserve blk IDs)\n",
    "parser = DrainParser(log_format, indir=input_dir, outdir=output_dir, rex=[], depth=5)\n",
    "parser.parse(log_file)\n",
    "\n",
    "# Step 2: Generate template ID map\n",
    "extract_templates_to_json(\n",
    "    os.path.join(output_dir, log_file + \"_templates.csv\"),\n",
    "    os.path.join(output_dir, \"hdfs_log_templates.json\")\n",
    ")\n",
    "\n",
    "# Step 3: Extract blk-based sessions\n",
    "extract_sessions(\n",
    "    os.path.join(output_dir, log_file + \"_structured.csv\"),\n",
    "    os.path.join(output_dir, \"hdfs_log_templates.json\"),\n",
    "    os.path.join(output_dir, \"hdfs_sequence.csv\")\n",
    ")\n",
    "\n",
    "# Step 4: Train/test split\n",
    "split_train_test(\n",
    "    sequence_csv=os.path.join(output_dir, \"hdfs_sequence.csv\"),\n",
    "    label_csv=os.path.join(input_dir, \"anomaly_label.csv\"),\n",
    "    output_dir=output_dir,\n",
    "    max_train=None  # auto-calculate if needed\n",
    ")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "structured_path = \"output_hdfs/HDFS.log_structured.csv\"\n",
    "df_structured = pd.read_csv(structured_path)\n",
    "\n",
    "print(f\"Structured log entries: {len(df_structured)}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class LogVocab:\n",
    "    def __init__(self, sequences, min_freq=1, max_size=None):\n",
    "        self.special_tokens = [\"<pad>\", \"<unk>\", \"<eos>\", \"<sos>\", \"<mask>\"]\n",
    "        self.counter = Counter()\n",
    "\n",
    "        for seq in tqdm(sequences, desc=\"Building vocab\"):\n",
    "            tokens = seq if isinstance(seq, list) else str(seq).strip().split()\n",
    "            self.counter.update(tokens)\n",
    "\n",
    "        self.itos = list(self.special_tokens)\n",
    "        if max_size:\n",
    "            max_size += len(self.special_tokens)\n",
    "\n",
    "        for token, freq in self.counter.most_common():\n",
    "            if freq < min_freq:\n",
    "                continue\n",
    "            if token not in self.itos:\n",
    "                self.itos.append(token)\n",
    "            if max_size and len(self.itos) >= max_size:\n",
    "                break\n",
    "\n",
    "        self.stoi = {tok: idx for idx, tok in enumerate(self.itos)}\n",
    "\n",
    "        self.pad_idx = self.stoi[\"<pad>\"]\n",
    "        self.unk_idx = self.stoi[\"<unk>\"]\n",
    "        self.eos_idx = self.stoi[\"<eos>\"]\n",
    "        self.sos_idx = self.stoi[\"<sos>\"]\n",
    "        self.mask_idx = self.stoi[\"<mask>\"]\n",
    "\n",
    "    def encode(self, sentence, seq_len=None, with_sos=False, with_eos=False):\n",
    "        tokens = sentence if isinstance(sentence, list) else str(sentence).strip().split()\n",
    "        indices = [self.stoi.get(tok, self.unk_idx) for tok in tokens]\n",
    "\n",
    "        if with_sos:\n",
    "            indices.insert(0, self.sos_idx)\n",
    "        if with_eos:\n",
    "            indices.append(self.eos_idx)\n",
    "\n",
    "        if seq_len:\n",
    "            indices = indices[:seq_len] + [self.pad_idx] * max(0, seq_len - len(indices))\n",
    "\n",
    "        return indices\n",
    "\n",
    "    def decode(self, indices, skip_pad=True):\n",
    "        tokens = [self.itos[i] for i in indices if not skip_pad or i != self.pad_idx]\n",
    "        return \" \".join(tokens)\n",
    "\n",
    "    def save(self, filepath):\n",
    "        with open(filepath, \"wb\") as f:\n",
    "            pickle.dump(self, f)\n",
    "\n",
    "    @staticmethod\n",
    "    def load(filepath):\n",
    "        with open(filepath, \"rb\") as f:\n",
    "            return pickle.load(f)\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.itos)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_file_path = \"output_hdfs/train\"\n",
    "\n",
    "with open(train_file_path, \"r\") as f:\n",
    "    sequences = [line.strip().split() for line in f if line.strip()]\n",
    "\n",
    "print(f\"Loaded {len(sequences)} sequences.\")\n",
    "\n",
    "vocab = LogVocab(sequences, min_freq=2)\n",
    "print(f\" Vocab built. Total tokens (including special): {len(vocab)}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "vocab.save(\"output_hdfs/vocab.pkl\")\n",
    "print(\"Vocab saved to output_hdfs/vocab.pkl\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def set_seed(seed=42):\n",
    "    random.seed(seed)\n",
    "    np.random.seed(seed)\n",
    "    torch.manual_seed(seed)\n",
    "    torch.cuda.manual_seed_all(seed)\n",
    "\n",
    "set_seed()\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open(\"output_hdfs/vocab.pkl\", \"rb\") as f:\n",
    "    vocab = pickle.load(f)\n",
    "print(f\"Vocabulary size: {len(vocab)}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "vocab.pad_index = vocab.stoi.get('<PAD>', 0)\n",
    "vocab.mask_index = vocab.stoi.get('<MASK>', 1)\n",
    "vocab.unk_index = vocab.stoi.get('<UNK>', 2)\n",
    "vocab.sos_index = vocab.stoi.get('<CLS>', 3)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class LogDataset(Dataset):\n",
    "    def __init__(self, log_seqs, time_seqs, vocab, seq_len=128, mask_ratio=0.15):\n",
    "        self.log_seqs = log_seqs\n",
    "        self.time_seqs = time_seqs\n",
    "        self.vocab = vocab\n",
    "        self.seq_len = seq_len\n",
    "        self.mask_ratio = mask_ratio\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.log_seqs)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        log_seq, time_seq = self.log_seqs[idx], self.time_seqs[idx]\n",
    "        input_ids, labels, time_input, time_labels = self.mask_sequence(log_seq, time_seq)\n",
    "\n",
    "        return {\n",
    "            \"input_ids\": torch.tensor(input_ids, dtype=torch.long),\n",
    "            \"labels\": torch.tensor(labels, dtype=torch.long),\n",
    "            \"time_input\": torch.tensor(time_input, dtype=torch.float).unsqueeze(1),\n",
    "            \"time_labels\": torch.tensor(time_labels, dtype=torch.float)\n",
    "        }\n",
    "\n",
    "    def mask_sequence(self, log_seq, time_seq):\n",
    "        tokens, labels = [], []\n",
    "        time_input, time_labels = [], []\n",
    "\n",
    "        for token, time in zip(log_seq, time_seq):\n",
    "            if random.random() < self.mask_ratio:\n",
    "                tokens.append(self.vocab.mask_index)\n",
    "                labels.append(self.vocab.stoi[token] if token in self.vocab.stoi else self.vocab.unk_index)\n",
    "                time_input.append(0)\n",
    "                time_labels.append(time)\n",
    "            else:\n",
    "                tokens.append(self.vocab.stoi[token] if token in self.vocab.stoi else self.vocab.unk_index)\n",
    "                labels.append(0)\n",
    "                time_input.append(time)\n",
    "                time_labels.append(0)\n",
    "\n",
    "        tokens = [self.vocab.sos_index] + tokens\n",
    "        labels = [self.vocab.pad_index] + labels\n",
    "        time_input = [0] + time_input\n",
    "        time_labels = [0] + time_labels\n",
    "\n",
    "        pad_len = self.seq_len - len(tokens)\n",
    "        tokens += [self.vocab.pad_index] * pad_len\n",
    "        labels += [self.vocab.pad_index] * pad_len\n",
    "        time_input += [0] * pad_len\n",
    "        time_labels += [0] * pad_len\n",
    "\n",
    "        return tokens[:self.seq_len], labels[:self.seq_len], time_input[:self.seq_len], time_labels[:self.seq_len]\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def fixed_window_parser(log_lines, window_size=20, min_len=10):\n",
    "    logkey_seqs, time_seqs = [], []\n",
    "    for line in log_lines:\n",
    "        line = line.strip().split()\n",
    "        if len(line) < min_len:\n",
    "            continue\n",
    "        keys, times = [], []\n",
    "        for item in line:\n",
    "            parts = item.split(\",\")\n",
    "            if len(parts) == 2:\n",
    "                keys.append(parts[0])\n",
    "                times.append(float(parts[1]))\n",
    "            else:\n",
    "                keys.append(parts[0])\n",
    "                times.append(0.0)\n",
    "        for i in range(0, len(keys), window_size):\n",
    "            logkey_seqs.append(keys[i:i + window_size])\n",
    "            time_seqs.append(times[i:i + window_size])\n",
    "    return logkey_seqs, time_seqs\n",
    "\n",
    "# Load raw log lines\n",
    "with open(\"/Users/poojithareddy/Desktop/ucf/Sem2/NLP/nlp_project/dataset/HDFS/HDFS_2k.log_structured.csv\", \"r\") as f:\n",
    "    raw_lines = f.readlines()\n",
    "\n",
    "# Generate sequences\n",
    "logkey_seqs, time_seqs = fixed_window_parser(raw_lines, window_size=20, min_len=10)\n",
    "\n",
    "# Split into train and validation sets\n",
    "logkey_train, logkey_valid, time_train, time_valid = train_test_split(\n",
    "    logkey_seqs, time_seqs, test_size=0.1, random_state=42\n",
    ")\n",
    "\n",
    "print(f\"Train sequences: {len(logkey_train)}, Valid sequences: {len(logkey_valid)}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create dataset instances\n",
    "train_dataset = LogDataset(logkey_train, time_train, vocab, seq_len=128)\n",
    "valid_dataset = LogDataset(logkey_valid, time_valid, vocab, seq_len=128)\n",
    "\n",
    "# Create PyTorch DataLoaders\n",
    "train_loader = DataLoader(train_dataset, batch_size=64, shuffle=True)\n",
    "valid_loader = DataLoader(valid_dataset, batch_size=64)\n",
    "\n",
    "# Check sample batch shape\n",
    "sample_batch = next(iter(train_loader))\n",
    "print(\"Sample input_ids shape:\", sample_batch[\"input_ids\"].shape)\n",
    "print(\"Sample time_input shape:\", sample_batch[\"time_input\"].shape)\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class SimpleBERT(nn.Module):\n",
    "    def __init__(self, vocab_size, hidden_dim=128, max_len=128):\n",
    "        super().__init__()\n",
    "        self.embed = nn.Embedding(vocab_size, hidden_dim, padding_idx=vocab.pad_index)\n",
    "        self.cls = nn.Sequential(\n",
    "            nn.Linear(hidden_dim, vocab_size),\n",
    "            nn.LogSoftmax(dim=-1)  # for NLLLoss\n",
    "        )\n",
    "    def forward(self, x):\n",
    "        x = self.embed(x)\n",
    "        cls_output = x[:, 0]  # [CLS] token\n",
    "        logits = self.cls(x)  # shape: [batch_size, seq_len, vocab_size]\n",
    "        return logits, cls_output\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_epoch(model, loader, optimizer, criterion):\n",
    "    model.train()\n",
    "    total_loss = 0\n",
    "    for batch in loader:\n",
    "        input_ids = batch[\"input_ids\"].to(device)\n",
    "        labels = batch[\"labels\"].to(device)\n",
    "\n",
    "        optimizer.zero_grad()\n",
    "        logits, _ = model(input_ids)\n",
    "        loss = criterion(logits.view(-1, logits.size(-1)), labels.view(-1))\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        total_loss += loss.item()\n",
    "    return total_loss / len(loader)\n",
    "def eval_epoch(model, loader, criterion):\n",
    "    model.eval()\n",
    "    total_loss = 0\n",
    "    with torch.no_grad():\n",
    "        for batch in loader:\n",
    "            input_ids = batch[\"input_ids\"].to(device)\n",
    "            labels = batch[\"labels\"].to(device)\n",
    "\n",
    "            logits, _ = model(input_ids)\n",
    "            loss = criterion(logits.view(-1, logits.size(-1)), labels.view(-1))\n",
    "            total_loss += loss.item()\n",
    "    return total_loss / len(loader)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = SimpleBERT(vocab_size=len(vocab)).to(device)\n",
    "optimizer = optim.Adam(model.parameters(), lr=1e-4)\n",
    "criterion = nn.NLLLoss(ignore_index=vocab.pad_index)\n",
    "train_losses, val_losses = [], []\n",
    "for epoch in range(10):\n",
    "    train_loss = train_epoch(model, train_loader, optimizer, criterion)\n",
    "    val_loss = eval_epoch(model, valid_loader, criterion)\n",
    "    train_losses.append(train_loss)\n",
    "    val_losses.append(val_loss)\n",
    "    print(f\"Epoch {epoch}: Train Loss = {train_loss:.4f}, Val Loss = {val_loss:.4f}\")\n",
    "torch.save(model.state_dict(), \"logbert_model.pth\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.plot(train_losses, label=\"Train Loss\")\n",
    "plt.plot(val_losses, label=\"Validation Loss\")\n",
    "plt.xlabel(\"Epoch\")\n",
    "plt.ylabel(\"Loss\")\n",
    "plt.legend()\n",
    "plt.title(\"Training vs Validation Loss\")\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open(\"output_hdfs/vocab.pkl\", \"rb\") as f:\n",
    "    vocab = pickle.load(f)\n",
    "\n",
    "vocab.pad_index = vocab.stoi.get('<PAD>', 0)\n",
    "vocab.mask_index = vocab.stoi.get('<MASK>', 1)\n",
    "vocab.unk_index = vocab.stoi.get('<UNK>', 2)\n",
    "vocab.sos_index = vocab.stoi.get('<CLS>', 3)\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def fixed_window(log_lines, window_size=20, min_len=10):\n",
    "    logkey_seqs, time_seqs = [], []\n",
    "    for line in log_lines:\n",
    "        line = line.strip().split()\n",
    "        if len(line) < min_len:\n",
    "            continue\n",
    "        keys, times = [], []\n",
    "        for item in line:\n",
    "            parts = item.split(\",\")\n",
    "            if len(parts) == 2:\n",
    "                keys.append(parts[0])\n",
    "                times.append(float(parts[1]))\n",
    "            else:\n",
    "                keys.append(parts[0])\n",
    "                times.append(0.0)\n",
    "        for i in range(0, len(keys), window_size):\n",
    "            logkey_seqs.append(keys[i:i + window_size])\n",
    "            time_seqs.append(times[i:i + window_size])\n",
    "    return logkey_seqs, time_seqs\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open(\"output_hdfs/test_normal\", \"r\") as f:\n",
    "    logkey_normal, time_normal = fixed_window(f.readlines(), window_size=20)\n",
    "\n",
    "with open(\"output_hdfs/test_abnormal\", \"r\") as f:\n",
    "    logkey_abnormal, time_abnormal = fixed_window(f.readlines(), window_size=20)\n",
    "\n",
    "print(\"Loaded:\", len(logkey_normal), \"normal sequences,\", len(logkey_abnormal), \"abnormal sequences\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class LogDataset(Dataset):\n",
    "    def __init__(self, log_seqs, time_seqs, vocab, seq_len=128, mask_ratio=0.15):\n",
    "        self.log_seqs = log_seqs\n",
    "        self.time_seqs = time_seqs\n",
    "        self.vocab = vocab\n",
    "        self.seq_len = seq_len\n",
    "        self.mask_ratio = mask_ratio\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.log_seqs)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        k, t = self.log_seqs[idx], self.time_seqs[idx]\n",
    "        tokens, labels, time_input, time_labels = self.mask_sequence(k, t)\n",
    "        return {\n",
    "            \"input_ids\": torch.tensor(tokens, dtype=torch.long),\n",
    "            \"labels\": torch.tensor(labels, dtype=torch.long),\n",
    "            \"time_input\": torch.tensor(time_input, dtype=torch.float).unsqueeze(1)\n",
    "        }\n",
    "\n",
    "    def mask_sequence(self, k, t):\n",
    "        tokens, labels, time_input, time_labels = [], [], [], []\n",
    "        for token, time in zip(k, t):\n",
    "            if np.random.rand() < self.mask_ratio:\n",
    "                tokens.append(self.vocab.mask_index)\n",
    "                labels.append(self.vocab.stoi.get(token, self.vocab.unk_index))\n",
    "                time_input.append(0)\n",
    "                time_labels.append(time)\n",
    "            else:\n",
    "                tokens.append(self.vocab.stoi.get(token, self.vocab.unk_index))\n",
    "                labels.append(0)\n",
    "                time_input.append(time)\n",
    "                time_labels.append(0)\n",
    "        tokens = [self.vocab.sos_index] + tokens\n",
    "        labels = [self.vocab.pad_index] + labels\n",
    "        time_input = [0] + time_input\n",
    "        time_labels = [0] + time_labels\n",
    "\n",
    "        pad_len = self.seq_len - len(tokens)\n",
    "        tokens += [self.vocab.pad_index] * pad_len\n",
    "        labels += [self.vocab.pad_index] * pad_len\n",
    "        time_input += [0] * pad_len\n",
    "        time_labels += [0] * pad_len\n",
    "\n",
    "        return tokens[:self.seq_len], labels[:self.seq_len], time_input[:self.seq_len], time_labels[:self.seq_len]\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class SimpleBERT(nn.Module):\n",
    "    def __init__(self, vocab_size, hidden_dim=128):\n",
    "        super().__init__()\n",
    "        self.embed = nn.Embedding(vocab_size, hidden_dim, padding_idx=vocab.pad_index)\n",
    "        self.cls = nn.Sequential(\n",
    "            nn.Linear(hidden_dim, vocab_size),\n",
    "            nn.LogSoftmax(dim=-1)\n",
    "        )\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = self.embed(x)\n",
    "        cls_output = x[:, 0]  \n",
    "        logits = self.cls(x)\n",
    "        return logits, cls_output\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_test_data(path, window_size=20):\n",
    "    with open(path, \"r\") as f:\n",
    "        lines = f.readlines()\n",
    "    return fixed_window(lines, window_size=window_size)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def detect_anomalies(model, loader, num_candidates=30, threshold_ratio=0.5):\n",
    "    model.eval()\n",
    "    total_sequences = 0\n",
    "    total_anomalies = 0\n",
    "\n",
    "    with torch.no_grad():\n",
    "        for batch in tqdm(loader):\n",
    "            inputs = batch[\"input_ids\"].to(device)\n",
    "            labels = batch[\"labels\"].to(device)\n",
    "\n",
    "            logits, _ = model(inputs) \n",
    "            top_preds = torch.topk(logits, num_candidates, dim=-1).indices\n",
    "\n",
    "            for i in range(inputs.size(0)):\n",
    "                masked_positions = labels[i] > 0\n",
    "                masked_tokens = labels[i][masked_positions]\n",
    "                predicted_top = top_preds[i][masked_positions]\n",
    "\n",
    "                if masked_tokens.numel() == 0:\n",
    "                    continue\n",
    "\n",
    "                num_undetected = sum(\n",
    "                    [masked_tokens[j].item() not in predicted_top[j].cpu().tolist() for j in range(masked_tokens.size(0))]\n",
    "                )\n",
    "\n",
    "                if num_undetected > threshold_ratio * masked_tokens.size(0):\n",
    "                    total_anomalies += 1\n",
    "\n",
    "                total_sequences += 1\n",
    "\n",
    "    return total_anomalies, total_sequences\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "model = SimpleBERT(vocab_size=len(vocab)).to(device)\n",
    "model.load_state_dict(torch.load(\"logbert_model.pth\", map_location=device))\n",
    "model.eval()\n",
    "\n",
    "normal_dataset = LogDataset(logkey_normal, time_normal, vocab)\n",
    "abnormal_dataset = LogDataset(logkey_abnormal, time_abnormal, vocab)\n",
    "\n",
    "normal_loader = DataLoader(normal_dataset, batch_size=64)\n",
    "abnormal_loader = DataLoader(abnormal_dataset, batch_size=64)\n",
    "\n",
    "print(\"Evaluating on normal logs...\")\n",
    "normal_anomalies, normal_total = detect_anomalies(model, normal_loader)\n",
    "\n",
    "print(\"Evaluating on abnormal logs...\")\n",
    "abnormal_anomalies, abnormal_total = detect_anomalies(model, abnormal_loader)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "normal_preds = evaluate_file(\"output_hdfs/test_normal\", vocab, model, threshold_ratio=0.5)\n",
    "abnormal_preds = evaluate_file(\"output_hdfs/test_abnormal\", vocab, model, threshold_ratio=0.5)\n",
    "\n",
    "TP = sum(abnormal_preds)\n",
    "FN = len(abnormal_preds) - TP\n",
    "FP = sum(normal_preds)\n",
    "TN = len(normal_preds) - FP\n",
    "\n",
    "P = 100 * TP / (TP + FP) if TP + FP > 0 else 0\n",
    "R = 100 * TP / (TP + FN) if TP + FN > 0 else 0\n",
    "F1 = 2 * P * R / (P + R) if P + R > 0 else 0\n",
    "\n",
    "print(f\"TP: {TP}, TN: {TN}, FP: {FP}, FN: {FN}\")\n",
    "print(f\"Precision: {P:.2f}%, Recall: {R:.2f}%, F1-Score: {F1:.2f}%\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import roc_auc_score, confusion_matrix, precision_score, recall_score, f1_score, accuracy_score\n",
    "\n",
    "# Combine predictions and ground truth\n",
    "y_true = [0] * len(normal_preds) + [1] * len(abnormal_preds)\n",
    "y_pred = normal_preds + abnormal_preds\n",
    "\n",
    "# Compute metrics\n",
    "accuracy = accuracy_score(y_true, y_pred)\n",
    "precision = precision_score(y_true, y_pred)\n",
    "recall = recall_score(y_true, y_pred)\n",
    "f1 = f1_score(y_true, y_pred)\n",
    "auc = roc_auc_score(y_true, y_pred)\n",
    "\n",
    "# Confusion matrix\n",
    "cm = confusion_matrix(y_true, y_pred)\n",
    "TP, FN, FP, TN = cm[1, 1], cm[1, 0], cm[0, 1], cm[0, 0]\n",
    "\n",
    "# Print results\n",
    "print(f\"Confusion Matrix:\\n{cm}\")\n",
    "print(f\"TP: {TP}, TN: {TN}, FP: {FP}, FN: {FN}\")\n",
    "print(f\"Accuracy:  {accuracy:.4f}\")\n",
    "print(f\"Precision: {precision:.4f}\")\n",
    "print(f\"Recall:    {recall:.4f}\")\n",
    "print(f\"F1 Score:  {f1:.4f}\")\n",
    "print(f\"AUC:       {auc:.4f}\")\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "logbert_env",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
